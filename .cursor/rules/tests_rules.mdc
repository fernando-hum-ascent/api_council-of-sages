---
description:
globs: **/tests/**/*.py
alwaysApply: false
---
# üìú Reglas para Tests

Este documento define los est√°ndares y buenas pr√°cticas para el desarrollo de pruebas en este repositorio. Su prop√≥sito es asegurar consistencia, calidad y mantenibilidad del c√≥digo de pruebas a trav√©s de gu√≠as claras para todos los desarrolladores del proyecto.

## üõ† Estructura de Proyecto

- **tests/**: Debe contener pruebas unitarias y de integraci√≥n que reflejen la estructura del c√≥digo de la aplicaci√≥n.
  - **conftest.py**: Define fixtures comunes para todas las pruebas.
  - **resources/**: Pruebas para los recursos de API.
  - **models/**: Pruebas para los modelos de datos.
  - **lib/**: Pruebas para servicios y l√≥gica compartida.
  - **orchestrator/**: Pruebas para los componentes del orquestador.
    - **tools/**: Pruebas para herramientas espec√≠ficas del orquestador.
  - **cassettes/**: Archivos de grabaci√≥n para pruebas que usan VCR.

---

## üß™ Convenciones de Nombrado

- Los nombres de archivos de prueba deben seguir el patr√≥n `test_<nombre_del_m√≥dulo>.py`.
- Las funciones de prueba deben seguir el patr√≥n `test_should_<resultado_esperado>_when_<condiciones>`.
- Los fixtures deben tener nombres descriptivos en snake_case que reflejen lo que proveen.
- Los directorios de pruebas deben reflejar exactamente la estructura del c√≥digo de la aplicaci√≥n.

---

## üìã Organizaci√≥n de Pruebas

- Usar **pytest** como framework principal de pruebas.
- Utilizar **pytest.mark.parametrize** para cubrir m√∫ltiples escenarios con una sola funci√≥n de prueba.
  - Los par√°metros deben agrupar escenarios l√≥gicamente relacionados. Por ejemplo:
    - Agrupar todos los casos donde el input es inv√°lido en un test parametrizado.
    - Agrupar todos los casos donde el input es v√°lido en otro test parametrizado.
  - Los tests parametrizados no deben contener condicionales internos, ya que esto sugiere que se est√°n probando escenarios conceptualmente distintos que deber√≠an estar en tests separados.
- Incluir pruebas para casos positivos (happy path) y negativos (manejo de errores).
- Aislar adecuadamente las pruebas para evitar interdependencias entre casos de prueba.
- Cuando se modifique el estado de un registro de base de datos a trav√©s de un endpoint o funci√≥n, se debe verificar el estado en el que qued√≥ el resgistro despu√©s del test:

  ```python
  async def test_update_user(client, user):
      req = {
          "status": "inactive",
      }
      resp = await client.patch(f'/users/{user.id}', json=req)
      assert resp.status_code == 200

      await user.async_reload()
      assert user.status is UserStatus.inactive
  ```

- Seguir el patr√≥n AAA (Arrange, Act, Assert) para una estructura clara:

  ```python
  # Arrange - Preparar datos y precondiciones
  input_data = {"key": "value"}

  # Act - Ejecutar la funci√≥n/m√©todo que se est√° probando
  result = function_under_test(input_data)

  # Assert - Verificar que el resultado es el esperado
  assert result == expected_output
  ```

---

## üîÑ Fixtures y Mocks

- Definir fixtures en **conftest.py** para configuraciones comunes:
  - A nivel de directorio para componentes espec√≠ficos
  - A nivel global para configuraciones comunes como conexiones a bases de datos
  - Los archivos **conftest.py** deben contener exclusivamente fixtures y hooks de pytest, nunca funciones o clases utilitarias
  - Las funciones o clases utilitarias para pruebas deben colocarse en archivos separados, generalmente **utils.py**
- Para pruebas con mocks, utilizar el siguiente orden de preferencia:
  1. Para pruebas que realizan peticiones HTTP a alg√∫n recurso externo, usar cassettes (VCR.py)
  2. Para pruebas que realizan peticiones HTTP de escenarios complicados de reproducir (errores 500, timeouts, network errors, etc.), usar respx, pytest_httpx, responses
  3. Para pruebas donde es necesario hacer patch de m√≥dulos espec√≠ficos de alguna biblioteca o partes del codebase, o que no pueden reproducirse f√°cilmente con las opciones anteriores, usar unittest.mock.patch, monkeypatch o similar
- **Datos de prueba (Dummy Data)**: Cuando se necesite crear datos de prueba o dummy data para m√∫ltiples tests:
  - Crear fixtures reutilizables en lugar de generar los datos directamente en cada test
  - Definir estos fixtures en el archivo **conftest.py** apropiado (local o global seg√∫n el alcance)
  - Usar nombres descriptivos que indiquen claramente qu√© tipo de datos proveen
  - Ejemplo:
    ```python
    @pytest.fixture
    def sample_user_data():
        """Proporciona datos de usuario de ejemplo para pruebas."""
        return {
            "id": 1,
            "name": "Test User",
            "email": "test@example.com",
            "status": "active"
        }

    @pytest.fixture
    def sample_car_data():
        """Proporciona datos de veh√≠culo de ejemplo para pruebas."""
        return {
            "id": "ABC123",
            "make": "Toyota",
            "model": "Corolla",
            "year": 2023
        }
    ```

---

## üíæ Gesti√≥n de Cassettes VCR

- Para pruebas que utilizan VCR:
  - No generar ni modificar manualmente los archivos de cassettes: se crean autom√°ticamente al ejecutar los tests.
  - Configurar VCR para filtrar datos sensibles de headers y payloads
  - Utilizar el fixture **vcr** proporcionado en conftest.py
  - Aplicar el decorador **@pytest.mark.vcr** a las pruebas que utilizan VCR

---

## üîç Assertions

- Utilizar assertions espec√≠ficas para diferentes tipos de validaci√≥n:
  - `assert result == expected` para igualdad exacta
  - `assert "text" in result` para coincidencias parciales
  - `assert len(result) == expected_count` para validar el n√∫mero exacto de elementos en una colecci√≥n
  - `assert result == expected_items` para validar el contenido exacto de colecciones cuando sea posible
  - `assert set(result) == set(expected_items)` cuando el orden no importa pero s√≠ el contenido exacto
- Incluir mensajes de error √∫tiles con las aserciones:

  ```python
  assert result == expected, f"Expected {expected}, got {result}"
  ```

- Validar coincidencias exactas para salidas deterministas y coincidencias de patrones para salidas variables

---

## üß© Pruebas de Herramientas (Tools)

- Las pruebas de tools del orquestador deben:
  - Verificar la entrada y salida de cada herramienta
  - Probar el manejo de errores y casos l√≠mite
  - Mockear dependencias externas cuando sea apropiado
  - Verificar la interacci√≥n correcta con el estado del orquestador
  - Comprobar que las respuestas cumplen con los formatos esperados

---

## üìù Documentaci√≥n

- Documentar configuraciones complejas de pruebas y casos no obvios.
- Agregar docstrings descriptivos para explicar el prop√≥sito de cada prueba:

  ```python
  def test_should_validate_user_input_when_data_is_complete():
      """
      Verifica que la validaci√≥n de entrada del usuario funcione correctamente
      cuando se proporcionan todos los campos requeridos con valores v√°lidos.
      """
  ```

- Mantener la documentaci√≥n actualizada cuando cambia la funcionalidad.

---

## ‚ö° Rendimiento

- Mantener las pruebas eficientes en tiempo de ejecuci√≥n:
  - Usar VCR Cassettes para en vez de mockear dependencias externas cuando sea posible
  - Agrupar pruebas relacionadas con fixtures comunes
  - Evitar pruebas redundantes o duplicadas
  - Preferir pruebas unitarias sobre pruebas de integraci√≥n para componentes no cr√≠ticos

---

## üõ°Ô∏è Buenas Pr√°cticas

- No escribir pruebas que solo validen funcionalidades nativas de los frameworks:
  - Evitar probar validadores nativos de Pydantic
  - Evitar probar funcionalidades est√°ndar de FastAPI
- Enfocarse en probar la l√≥gica de negocio propia
- Mantener alta cobertura en componentes cr√≠ticos
- Ejecutar pruebas mediante `make test` o `make testcov`
- Aplicar Cassettes para aislar el c√≥digo de servicios externos
- Verificar que las pruebas fallen cuando deben fallar (cambiar temporalmente el c√≥digo para verificar)

---

## üìå Aplicaci√≥n obligatoria

- Mantener el tiempo de ejecuci√≥n de pruebas bajo
- Agregar nuevas pruebas al implementar nuevas funcionalidades
- Actualizar las pruebas existentes al modificar funcionalidades
